# ğŸ¤ DSP-Speech-Command-Recognition

> A **Distance-Based Speech Recognition System** using classical Digital Signal Processing (DSP) techniques â€” no deep learning required.

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Platform](https://img.shields.io/badge/Platform-Local%20%7C%20Colab%20%7C%20Kaggle-orange.svg)](#-quick-start)

---

## ğŸ“‹ Overview

This project implements a complete speech recognition pipeline for classifying **8 spoken commands** using traditional DSP methods. It demonstrates fundamental signal processing concepts without relying on neural networks or deep learning frameworks.

### ğŸ¯ Supported Commands

| | | | |
|:---:|:---:|:---:|:---:|
| `down` | `go` | `left` | `no` |
| `right` | `stop` | `up` | `yes` |

---

## âœ¨ Features

- **ğŸ”§ Complete DSP Pipeline** â€” From raw audio to classification
- **ğŸ“Š No Deep Learning** â€” Pure signal processing approach
- **ğŸŒ Cross-Platform** â€” Runs on Local, Google Colab, and Kaggle
- **ğŸ’¾ Smart Caching** â€” Preprocessed data saved for fast reloading
- **ğŸ“ˆ Rich Visualizations** â€” Average waveforms, magnitude spectra, spectrograms, and comparative views
- **ğŸ¯ Per-Class Templates** â€” Average frame computation for each speech command
- **ğŸ“‰ Statistical Analysis** â€” Energy, peak magnitude, and peak frequency metrics
- **ğŸ§¹ Clean Code** â€” Follows SOLID, DRY, and KISS principles

---

## ğŸ—ï¸ Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           SPEECH RECOGNITION PIPELINE                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Audio   â”‚â”€â”€â”€â–¶â”‚ Preprocessing â”‚â”€â”€â”€â–¶â”‚ Framing â”‚â”€â”€â”€â–¶â”‚   FFT &     â”‚â”€â”€â”€â–¶â”‚  Average    â”‚ â”‚
â”‚  â”‚  Input   â”‚    â”‚              â”‚    â”‚         â”‚    â”‚  Spectrum   â”‚    â”‚  Frames     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚                â”‚                  â”‚                â”‚                 â”‚          â”‚
â”‚       â–¼                â–¼                  â–¼                â–¼                 â–¼          â”‚
â”‚   .wav files     â€¢ DC removal       â€¢ 25ms frames     â€¢ Magnitude       â€¢ Per-class    â”‚
â”‚   16kHz mono     â€¢ Pre-emphasis     â€¢ 10ms hop          Spectrum          templates    â”‚
â”‚   1 second       â€¢ Normalization    â€¢ Hamming window  â€¢ rfft()          â€¢ Waveforms    â”‚
â”‚                  â€¢ Deduplication                                        â€¢ Spectrograms â”‚
â”‚                                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Option 1: Google Colab
1. Upload the notebook to Colab
2. Run all cells

### Option 2: Local Installation

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/DSP-Speech-Command-Recognition.git
cd DSP-Speech-Command-Recognition

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run the notebook
jupyter notebook speech_recognition_pipeline.ipynb
```

### Option 3: Kaggle
1. Upload the notebook to Kaggle
2. Enable Internet access in notebook settings
3. Run all cells

---

## ğŸ“ Project Structure

```
DSP-Speech-Command-Recognition/
â”‚
â”œâ”€â”€ ğŸ““ speech_recognition_pipeline.ipynb  # Main notebook (self-contained)
â”œâ”€â”€ ğŸ“„ README.md                          # This file
â”œâ”€â”€ ğŸ“„ requirements.txt                   # Python dependencies
â”œâ”€â”€ ğŸ“„ LICENSE                            # MIT License
â”‚
â”œâ”€â”€ ğŸ“‚ mini_speech_commands/              # Dataset (auto-downloaded)
â”‚   â””â”€â”€ ğŸ“‚ mini_speech_commands/
â”‚       â”œâ”€â”€ ğŸ“‚ down/
â”‚       â”œâ”€â”€ ğŸ“‚ go/
â”‚       â”œâ”€â”€ ğŸ“‚ left/
â”‚       â”œâ”€â”€ ğŸ“‚ no/
â”‚       â”œâ”€â”€ ğŸ“‚ right/
â”‚       â”œâ”€â”€ ğŸ“‚ stop/
â”‚       â”œâ”€â”€ ğŸ“‚ up/
â”‚       â””â”€â”€ ğŸ“‚ yes/
â”‚
â”œâ”€â”€ ğŸ“‚ processed_data/                    # Cached preprocessed data
    â”œâ”€â”€ preprocessed_data.npz
    â””â”€â”€ metadata.json

```

---

## ğŸ“Š Dataset

This project uses the **Mini Speech Commands** dataset from Google:

| Property | Value |
|----------|-------|
| **Source** | [TensorFlow Speech Commands](https://www.tensorflow.org/datasets/catalog/speech_commands) |
| **Samples** | 8,000 total (1,000 per class) |
| **Classes** | 8 speech commands |
| **Format** | WAV, 16-bit PCM |
| **Sample Rate** | 16,000 Hz |
| **Duration** | ~1 second per sample |

The dataset is automatically downloaded when you run the notebook.

---

## ğŸ”¬ Technical Details

### Preprocessing Pipeline

| Step | Description | Purpose |
|------|-------------|---------|
| **1. Duplicate Removal** | MD5 hash comparison | Remove redundant samples |
| **2. DC Offset Removal** | Subtract mean | Center signal at zero |
| **3. Length Normalization** | Pad/truncate to 16,000 samples | Ensure uniform length |
| **4. Pre-emphasis** | `y[n] = x[n] - 0.97Â·x[n-1]` | Boost high frequencies |
| **5. Amplitude Normalization** | Scale to [-1, 1] | Standardize amplitude range |

### Framing Parameters

| Parameter | Value | Samples |
|-----------|-------|---------|
| Frame Length | 25 ms | 400 |
| Hop Length | 10 ms | 160 |
| Overlap | 60% | 240 |
| Window | Hamming | â€” |
| Frames/Sample | 98 | â€” |

---

## ğŸ“ˆ Results

### Dataset Statistics After Preprocessing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PREPROCESSING RESULTS          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Original samples:     8,000           â”‚
â”‚  Duplicates removed:      27           â”‚
â”‚  Final samples:        7,973           â”‚
â”‚  Total frames:       781,354           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Samples per class:   ~996-998         â”‚
â”‚  Dataset balanced:    âœ… Yes           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Technical Implementation Details

### Train / Test Split 
```python
X_train_frames, X_test_frames, y_train, y_test = train_test_split(
    frames_data, labels, test_size=0.20, random_state=42, stratify=labels
)
```

### FFT Feature Extraction 
```python
X_train_spectrum = np.abs(np.fft.rfft(X_train_frames, axis=2)).astype(np.float32)
X_test_spectrum  = np.abs(np.fft.rfft(X_test_frames,  axis=2)).astype(np.float32)
# Output: (6378, 98, 201) â†’ 98 frames Ã— 201 frequency bins
```

### Average Frames Computation
```python
# Compute average frames per class for template creation
for class_label in unique_classes:
    class_mask = (y_train == class_label)
    class_frames = X_frames_train[class_mask]
    avg_frames_per_class[idx] = np.mean(class_frames, axis=0)
# Output: (8, 98, 400) â†’ 8 classes Ã— 98 frames Ã— 400 samples per frame
```

### Visualizations Generated

| Visualization | Description |
|--------------|-------------|
| **Average Waveforms** | Overlap-add reconstructed waveforms from averaged frames |
| **Magnitude Spectra** | FFT-based frequency domain representation per class |
| **Spectrograms** | Time-frequency 2D heatmaps for each speech command |
| **Comparative View** | Overlaid waveforms and spectra for cross-class comparison |

### Summary Statistics
For each class, the following metrics are computed:
- **Frame Energy** â€” Sum of squared sample values
- **Peak Magnitude** â€” Maximum spectral amplitude
- **Peak Frequency** â€” Frequency bin with highest energy (Hz)


## ï¸ Requirements

- Python 3.8+
- NumPy
- SciPy
- Matplotlib

See [`requirements.txt`](requirements.txt) for exact versions.

---

## ğŸ—ºï¸ Roadmap

- [x] **Stage 1:** Data Loading & Verification
- [x] **Stage 2:** Preprocessing & Framing
- [x] **Stage 3:** Feature Extraction (FFT Magnitude Spectrum)
- [x] **Stage 4:** Template Creation (Average Frames per Class)
- [ ] **Stage 5:** Distance Metrics (DTW, Euclidean)
- [ ] **Stage 6:** Classification & Evaluation
- [ ] **Stage 7:** Real-time Demo

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Yussof Waleed**
- GitHub: [@Yussof-waleed](https://github.com/Yussof-Waleed)
- University: Helwan University - Level 4 - DSP Course

---

